{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that the \"DATE\" column's format in different feature csv data being different from different online sources, the above merge would not be able to happen properly, resulting the dataframe to only have the merged column names but no tuple data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASPNHSUS</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>USACPIHOUMINMEI</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>CSUSHPINSA</th>\n",
       "      <th>MSPNHSUS</th>\n",
       "      <th>MSACSR</th>\n",
       "      <th>MORTGAGE30US</th>\n",
       "      <th>NHFSEPUC</th>\n",
       "      <th>NHSDPC</th>\n",
       "      <th>NHSDPNS</th>\n",
       "      <th>HNFSEPUSSA</th>\n",
       "      <th>HSN1F</th>\n",
       "      <th>DSPIC96</th>\n",
       "      <th>TOTALSA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144200.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>59.963759</td>\n",
       "      <td>4.03</td>\n",
       "      <td>75.697</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>8.4320</td>\n",
       "      <td>131.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>6616.3</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144800.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>60.195578</td>\n",
       "      <td>4.06</td>\n",
       "      <td>75.652</td>\n",
       "      <td>117200.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8.7625</td>\n",
       "      <td>129.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>6649.9</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>144800.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>60.466033</td>\n",
       "      <td>3.98</td>\n",
       "      <td>75.812</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.9350</td>\n",
       "      <td>135.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>6659.6</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145000.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>60.388760</td>\n",
       "      <td>3.73</td>\n",
       "      <td>76.079</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.8525</td>\n",
       "      <td>134.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>6679.4</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>146000.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>60.350123</td>\n",
       "      <td>3.82</td>\n",
       "      <td>76.398</td>\n",
       "      <td>113000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.6720</td>\n",
       "      <td>135.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>6712.9</td>\n",
       "      <td>13.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ASPNHSUS  UNRATE  USACPIHOUMINMEI  FEDFUNDS  CSUSHPINSA  MSPNHSUS  MSACSR  \\\n",
       "0  144200.0     7.3        59.963759      4.03      75.697  120000.0     5.2   \n",
       "1  144800.0     7.4        60.195578      4.06      75.652  117200.0     4.9   \n",
       "2  144800.0     7.4        60.466033      3.98      75.812  120000.0     6.1   \n",
       "3  145000.0     7.4        60.388760      3.73      76.079  120000.0     6.1   \n",
       "4  146000.0     7.6        60.350123      3.82      76.398  113000.0     6.0   \n",
       "\n",
       "   MORTGAGE30US  NHFSEPUC  NHSDPC  NHSDPNS  HNFSEPUSSA  HSN1F  DSPIC96  \\\n",
       "0        8.4320     131.0    16.0     17.0       281.0  676.0   6616.3   \n",
       "1        8.7625     129.0    16.0     20.0       269.0  639.0   6649.9   \n",
       "2        8.9350     135.0    16.0     20.0       279.0  553.0   6659.6   \n",
       "3        8.8525     134.0    14.0     17.0       274.0  546.0   6679.4   \n",
       "4        8.6720     135.0    16.0     18.0       273.0  554.0   6712.9   \n",
       "\n",
       "   TOTALSA  \n",
       "0     12.6  \n",
       "1     12.9  \n",
       "2     12.8  \n",
       "3     12.6  \n",
       "4     13.1  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thus, the following is a new way of joining our data into one single dataframe object.\n",
    "# Find all names of our data files in csv format (all csv data has the format of 2 columns, 1st column being DATE, \n",
    "# Second column being the feature we believe that is important for our model). In order to merge all these seperate\n",
    "# featrue values into a single csv (pandas DataFrame) file for futher data cleaning and processing before the modeling,\n",
    "# phase, we loop through all the file names (saved into a list) and reading them into the pandas DataFrame one at a \n",
    "# time while joining them based on the \"DATE\" column.\n",
    "\n",
    "# Initialize an empty DataFrame object to build up into a complete dataframe with all needed features through merging other feature dataframes\n",
    "df = pd.DataFrame()\n",
    "# locate the directory path where the data are located on the local machine\n",
    "wd = os.path.abspath('FeatureData')\n",
    "# find all the data in csv format under the located directory and save them as a list variable\n",
    "all_files = glob.glob(wd + '/*.csv')\n",
    "# Open all the csv feature data as a Pandas DataFrame and saving it inside a list variable\n",
    "df_list = [pd.read_csv(file) for file in all_files]\n",
    "# Expand the initialized DataFrame by assigning it the second column of all the feature data file as a new column\n",
    "# omitting the DATE column to avoid problems caused by different DATE format\n",
    "for fileIndex in range(len(df_list)):\n",
    "    df[df_list[fileIndex].columns[1]] = df_list[fileIndex][df_list[fileIndex].columns[1]]\n",
    "# Add the DATE column into the expanded dataframe with the correct format (the same as our training and testing dataset)\n",
    "##### Continued on the next few cells #####\n",
    "\n",
    "df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The format of DATES in the training and testing data sets provided by synchrony FINANCIAL are in the for of:\n",
    "#                                                  '01/month/year'\n",
    "# As all other feature data are found with a corresponding DATE with Month being from January (01) ~ December (12)\n",
    "# and Year from 1992 (92) ~ 2017 (17) and with the first day of the month (01).\n",
    "# Thus, the next step is to create a pandas Series object representing the DATE in the corresponding string format\n",
    "\n",
    "# create the month list with strings representing 1 ~ 12 with a '0' prefix\n",
    "month = ['0%d' % s for s in range(1,13) ]\n",
    "# create a new list with the substring constructed by the last two characters in the string to fit the month string \n",
    "# representation in the testin and training data sets and we are done for the month\n",
    "months = [m[-2:] for m in month]\n",
    "months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As above with the month, we do the similar process with the year.\n",
    "# creat a list of string representation of the year with range from 1992 ~ 2017\n",
    "year = [str(y) for y in range(1992, 2018)]\n",
    "# create the years list with the correct format of the year's strings by saving all the year element's substring \n",
    "# constructed by the last two characters of the string\n",
    "years = [y[-2:] for y in year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the DATE list with the same string formats as the training and testing data set\n",
    "# by looping through the months and years list \n",
    "DATE = ['01/%s/%s' % (month, year) for year in years for month in months]\n",
    "# Turn the list into a Pandas Series Object so we can join it with the main dataframe\n",
    "DATE = pd.Series(DATE)\n",
    "# Join the DATE Series with the main dataframe, so the dataframe would have the \"DATE\" column\n",
    "df[\"DATE\"] = DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASPNHSUS</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>USACPIHOUMINMEI</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>CSUSHPINSA</th>\n",
       "      <th>MSPNHSUS</th>\n",
       "      <th>MSACSR</th>\n",
       "      <th>MORTGAGE30US</th>\n",
       "      <th>NHFSEPUC</th>\n",
       "      <th>NHSDPC</th>\n",
       "      <th>NHSDPNS</th>\n",
       "      <th>HNFSEPUSSA</th>\n",
       "      <th>HSN1F</th>\n",
       "      <th>DSPIC96</th>\n",
       "      <th>TOTALSA</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>369200.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>120.448724</td>\n",
       "      <td>1.16</td>\n",
       "      <td>194.684</td>\n",
       "      <td>314200.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.880</td>\n",
       "      <td>176.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>12785.4</td>\n",
       "      <td>16.4</td>\n",
       "      <td>01/08/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>379300.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>120.728451</td>\n",
       "      <td>1.15</td>\n",
       "      <td>195.155</td>\n",
       "      <td>331500.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3.805</td>\n",
       "      <td>175.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>12786.9</td>\n",
       "      <td>18.9</td>\n",
       "      <td>01/09/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>394000.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>121.139930</td>\n",
       "      <td>1.15</td>\n",
       "      <td>195.430</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.895</td>\n",
       "      <td>176.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>12805.3</td>\n",
       "      <td>18.4</td>\n",
       "      <td>01/10/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>388500.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>121.279407</td>\n",
       "      <td>1.16</td>\n",
       "      <td>195.815</td>\n",
       "      <td>343400.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.922</td>\n",
       "      <td>181.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>12814.8</td>\n",
       "      <td>17.9</td>\n",
       "      <td>01/11/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>398700.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>121.604340</td>\n",
       "      <td>1.30</td>\n",
       "      <td>196.216</td>\n",
       "      <td>340100.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.950</td>\n",
       "      <td>176.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>12846.3</td>\n",
       "      <td>18.2</td>\n",
       "      <td>01/12/17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ASPNHSUS  UNRATE  USACPIHOUMINMEI  FEDFUNDS  CSUSHPINSA  MSPNHSUS  \\\n",
       "307  369200.0     4.4       120.448724      1.16     194.684  314200.0   \n",
       "308  379300.0     4.2       120.728451      1.15     195.155  331500.0   \n",
       "309  394000.0     4.1       121.139930      1.15     195.430  319500.0   \n",
       "310  388500.0     4.1       121.279407      1.16     195.815  343400.0   \n",
       "311  398700.0     4.1       121.604340      1.30     196.216  340100.0   \n",
       "\n",
       "     MSACSR  MORTGAGE30US  NHFSEPUC  NHSDPC  NHSDPNS  HNFSEPUSSA  HSN1F  \\\n",
       "307     6.0         3.880     176.0    16.0     13.0       280.0  559.0   \n",
       "308     5.3         3.805     175.0    17.0     14.0       280.0  639.0   \n",
       "309     5.6         3.895     176.0    18.0     13.0       286.0  616.0   \n",
       "310     4.8         3.922     181.0    17.0     16.0       287.0  711.0   \n",
       "311     5.4         3.950     176.0    19.0     13.0       295.0  653.0   \n",
       "\n",
       "     DSPIC96  TOTALSA      DATE  \n",
       "307  12785.4     16.4  01/08/17  \n",
       "308  12786.9     18.9  01/09/17  \n",
       "309  12805.3     18.4  01/10/17  \n",
       "310  12814.8     17.9  01/11/17  \n",
       "311  12846.3     18.2  01/12/17  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with feature data collected in a seasonal matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have a few features which are recorded in a seasonal style, thus we will expand them so that the data for each\n",
    "# sesaon would be the same for each months in that season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to work with the Seasonal Data, need to change it into monthly presentation (Each tuple data times three)\n",
    "# We solve it by replicating each row of data 3 times and saving these replicated data into a new list\n",
    "# then transform the list into a pandas Series. This would expand the seasonal data into monthly data.\n",
    "# then we simply join this series into the main dataframe as a new column\n",
    "\n",
    "# change into the directory where the seasonal Data featrues csv data files are located\n",
    "wd = os.path.abspath(\"Seasonal Data to be changed\")\n",
    "# save all the csv files in the files_list\n",
    "files_list = glob.glob( wd + '/*.csv')\n",
    "# read all files into an dataframe and save dataframes to the df_list\n",
    "df_list = [pd.read_csv(file) for file in files_list]\n",
    "# create the data_list where we will put the monthly data expanded by the seasonal data in\n",
    "data_list = []\n",
    "# loop through each of the pandas dataframe to access the feature data Series (all based on the column with index 1)\n",
    "for i in range(len(df_list)):\n",
    "    seasonal_data_series = df_list[i][df_list[i].columns[1]]\n",
    "    # for the data values in these data seriesexpand each of them (appending) 3 times into the data_list\n",
    "    for data in seasonal_data_series:\n",
    "        for expand in range(3):\n",
    "            data_list.append(data)\n",
    "    # create a pandas Series from the data_list\n",
    "    new_data_series = pd.Series(data_list)\n",
    "    # Add the new pandas series into the main dataframe as a new column\n",
    "    df[df_list[i].columns[1]] = new_data_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the dataframe from memory to local disk as \"AllFeaturesData.csv\"\n",
    "df.to_csv(\"AllFeaturesData.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in the target variable to the main data file.\n",
    "# To do so we append the 2 feature files of training and testing dataset, appending testing data set after the training\n",
    "# data set so that the DATE order aligns with the main merged feature data file.\n",
    "\n",
    "# read in the csv file we which we used OpenRefine to change all string representation of numbers to numeric data\n",
    "df = pd.read_csv(\"AllFeaturesData-csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change into the directory where the Testing and Training data are located\n",
    "wd = os.path.abspath(\"TrainTestingData\")\n",
    "# save all the excel (.xlsm) files in the files_list\n",
    "files_list = glob.glob( wd + '/*.xlsm')\n",
    "# Individually pick out the training and testing dataset\n",
    "files_list[1] # Training dataset with DATES from (1992/01/01 ~ 2015/12/01)\n",
    "files_list[0] # Testing Dataset with DATES from (2016/01/01 ~ 2017/12/01)\n",
    "# read them into dataframes\n",
    "df_train = pd.read_excel(files_list[1])\n",
    "df_test = pd.read_excel(files_list[0])\n",
    "# Instantiate an empty dataframe used to append both training and testing data set to it to extract the target variable's\n",
    "# value as pandas series object for merging into the main dataframe later\n",
    "df_main = pd.DataFrame()\n",
    "df_main = df_main.append(df_train)\n",
    "df_main = df_main.append(df_test)\n",
    "# reindex to to get rid of duplicated index so we can merge the needed feature column to the main dataframe\n",
    "df_main = df_main.reset_index()\n",
    "# Merge it into the main dataframe, and change its type to integer\n",
    "df[df_main.columns[2]] = df_main[df_main.columns[2]]\n",
    "map(int,df[df_main.columns[2]])\n",
    "\n",
    "\n",
    "# Save dataframe to \"AllFeaturesData-csv.csv\", ommitting its index\n",
    "df.to_csv(\"AllFeaturesData-csv.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep copy the original dataframe into another dataframe named df2 for splitting it into the training and testing data\n",
    "# set based on the index (training set has the DATE starting from (1992/01/01 ~ 2015/12/01)\n",
    "# while the testing set has the DATE starting from (2016/01/01 ~ 2017/12/01))\n",
    "df2 = df.copy()\n",
    "# The dataframe for the training dataset with index 0 to the last index of 12 * (2017 - 2015) [since each year has 12\n",
    "# months, meaning for the last two years, which belong in the testing set has 12 * 2Years of rows(which is also\n",
    "# the numbers of index)]. Similar with the testing set with the last 12*(2017-2015) rows (number of index)\n",
    "df_train = df2[:-12*(2017-2015)]\n",
    "df_test = df2[-12*(2017-2015):]\n",
    "# Next we have to save these two dataframes into a CSV file under the \"TrainTestingData\" directory where the original\n",
    "# test and train data sets are (though with only the DATE and Sales in $MM features)\n",
    "\n",
    "# Save the current working directory path in the variable current_working_directory so that after changing into the \n",
    "# \"TrainTestingData\" directory and saving the files we can change back into this directory to avoid bugs created by\n",
    "# still being in the \"TrainTestingData\" directory if we rerun this program several times\n",
    "current_working_directory = os.path.abspath(os.curdir)\n",
    "\n",
    "# Find the path of the directory \"TrainTestingData\"\n",
    "Train_Testing_Data_Path = os.path.abspath(\"TrainTestingData\")\n",
    "# change the working directory to that path\n",
    "os.chdir(Train_Testing_Data_Path)\n",
    "# save the dataframes to the directory\n",
    "df_train.to_csv(\"training_set_with_features.csv\", index = False)\n",
    "df_test.to_csv(\"testing_set_with_features.csv\", index = False)\n",
    "\n",
    "# Change back into our previous directory where it all started\n",
    "os.chdir(current_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
